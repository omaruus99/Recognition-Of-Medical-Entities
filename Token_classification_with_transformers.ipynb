{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jACMr6-7mVX0"
      },
      "source": [
        "\n",
        "Tout d'abord, installer les librairies nécessaires: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z8KKeF8qht9"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers datasets evaluate seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6G2PhAYmavw"
      },
      "source": [
        "Charger les libraries dans l'environnement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUcLQmW4qzwx"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "import zipfile\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5upOiFtoS5l"
      },
      "source": [
        "Uploader dans votre environment de travail l'archive zip fournie (corpusCasM2-main.zip) à l'aide de l'explorateur de fichier google colab.\n",
        "\n",
        "Puis décompresser le fichier avec la commande suivante: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by_1Go0VnsiH"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(\"corpusCasM2-main.zip\", \"r\") as archive:\n",
        "        archive.extractall(\"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObZWHpjCrEa2"
      },
      "source": [
        "Charger le dataset corpusCasM2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f1zOAChq3v3"
      },
      "outputs": [],
      "source": [
        "corpusCas = load_dataset(\"data/corpusCasM2-main/corpusCasM2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQhWypsIrbta"
      },
      "source": [
        "Afficher un exemple du trainset:\n",
        "\n",
        "id\n",
        ": correspond à l'id du document et le numero de la phrase dans le document \n",
        "\n",
        "tokens\n",
        ": contient le texte de la phrase pre-tokenisé\n",
        "\n",
        "ner_tags\n",
        ": contient les labels au format [BIO](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)). Ils sont affichés ici par leur ID "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clDNVdiDq4d7"
      },
      "outputs": [],
      "source": [
        "corpusCas[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qc6hPMerhYL"
      },
      "source": [
        "Il est possible de récupérer la liste des labels à partir des features du dataset: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3_3WRguq8Sk"
      },
      "outputs": [],
      "source": [
        "label_list = corpusCas[\"train\"].features[\"ner_tags\"].feature.names\n",
        "label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go0HZCNJ_3z2"
      },
      "source": [
        "Télécharger le tokenizer du modèle choisi (ici \"distilbert-base-uncased\"): "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSTpjjyjrvoy"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kww0Eq74ov5Q"
      },
      "source": [
        "Voici un exemple d'output du tokenizer: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAHbDylhE2R3"
      },
      "outputs": [],
      "source": [
        "example = corpusCas[\"train\"][0]\n",
        "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEe-qIlMo0iM"
      },
      "source": [
        "Vous pouvez constater que le texte tokenisé comprend plus de tokens que de mots initialement présents. Or les labels n'existent que pour un mot. Il faut donc réaligner les tokens et les labels en utilisant la fonction suivante: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4n5uQhQAT8V"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljjRbG3yA2yF"
      },
      "outputs": [],
      "source": [
        "tokenized_corpus = corpusCas.map(tokenize_and_align_labels, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foTbL8F9qIRY"
      },
      "source": [
        "On obtient bien l'effet escompté: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dktf4_-bA7pl"
      },
      "outputs": [],
      "source": [
        "tokenized_corpus[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R8WQI2iqQWu"
      },
      "source": [
        "On crée ensuite un [DataCollator](https://huggingface.co/docs/transformers/main_classes/data_collator) qui va servir à créer les batchs en entrée du modèle. On utilise un DataCollator spécifique à la tache "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhlaD7ebA-10"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7B7G4nlq3ut"
      },
      "source": [
        "Ensuite on crée une petite fonction qui va être chargée de faire l'évaluation du modèle: ici on se sert de la métrique [seqeval](https://huggingface.co/spaces/evaluate-metric/seqeval).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_Zb3pkNEP7j"
      },
      "outputs": [],
      "source": [
        "\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuj6JbLfrTa5"
      },
      "source": [
        "Créons à présent les dictionnaires qui permettent de lier les labels avec leurs IDs respectifs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAs_FuZ0EdGz"
      },
      "outputs": [],
      "source": [
        "id2label = {i:label for i,label in enumerate(label_list)}\n",
        "label2id = {v:k for k,v in id2label.items()}\n",
        "label2id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvySE6slrogF"
      },
      "source": [
        "Nous pouvons à présent instancier le modèle que nous voulons fine-tuner: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xa408u4lFFmZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=len(id2label), id2label=id2label, label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0olKJj5DsWOq"
      },
      "source": [
        "## TODO\n",
        "\n",
        "Pour simplifier le travail, les **scopes de temporalité ne sont pas pris en compte** dans cette version du dataset. \n",
        "\n",
        "1. Identifier un modèle adapté à la tâche. Ici nous voulons faire de la reconnaissance d'entités nommées (classification de tokens) dans des textes en français. Ce choix devra être argumenté. Dans l'exemple, le modèle choisi n'est pas particulièrement adapté au français. \n",
        "\n",
        "2. Fine-tuner ce modèle sur la tâche. C'est à dire réentrainer le modèle avec les données du corpusCasM2. Vous devrez tester différentes combinaisons d'hyperparamètres afin de trouver les meilleurs. Pour cela, vous mettrez en place une stratégie d'optimisation des hyperparamètres que vous justifierez.\n",
        "\n",
        "3. Evaluer de manière adaptée et argumentée les résultats obtenus.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6 (main, Aug 30 2022, 05:12:36) [Clang 13.1.6 (clang-1316.0.21.2.5)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
