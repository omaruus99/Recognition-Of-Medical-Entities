# -*- coding: utf-8 -*-
"""LSTM_classification_student.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/aneuraz/intro-keras/blob/master/LSTM_classification_student.ipynb

# Classification de texte en deep learning (LSTM et convolution) 

## But de la tâche 

A partir d'un dataset d'articles PUBMED, le but est de classifier les articles dans des catégories thématiques en fonction de leur titre. 

Après une phase de préprocessing du texte, nous entrainerons un modèle à base de convolutions, puis un modèle à base de réseau de neurones récurrents (LSTM)

## Cloner le repo https://github.com/aneuraz/intro-keras.git
"""

!git clone https://github.com/aneuraz/intro-keras.git

"""## Import des libraries"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import json 
import tensorflow as tf
import numpy as np

"""## Chargement des données

Toutes les données chargées se situent dans le répertoire `/content/`.
Les données sont dans un fichier JSON.
"""

with open('/content/intro-keras/ai_pub_samp.json','r') as f:
  data = json.load(f)

data[0]

"""## TODO: Extraire les titres et les catégories"""

# mettre le titre en minuscule dans la variable x

X=[]
for ex in data :
  X.append(ex['title'].lower())


Y = [cat["categories"][0] for cat in data]

print (Y)

"""## TODO: Calculer la longueur maximale des titres dans le dataset"""

# longueur maximale des titres, variable max_len

"""## TODO: Diviser le dataset en train (X_train, Y_train) et test (X_test, Y_test)"""

from sklearn.model_selection import train_test_split
# X_train, Y_train
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0, train_size=0.8)

"""## Transformer la variable Y en vecteur numerique

["Cat 1", "Cat 2"] -> [0, 1]
"""

# creer un mapping cat_2_id
cat_2_id = {}
for cat in Y_train : 
  if cat not in cat_2_id:
    cat_2_id[cat] = len(cat_2_id)

# creer un reverse mapping id_2_cat (inverser la clé et la valeur qu'on vient de creer)
id_2_cat = {value:key for key, value in cat_2_id.items()}

# calculer la taille du vocabulaire cat_vocab
cat_vocab = len(id_2_cat)

# preprocesser les X_train et X_test en X_train_id et X_test_id

def preprocess_y(Y, cat_2_id):
  result = []
  for ex in Y:
    if ex not in cat_2_id:
      result.append(len(cat_2_id))
    else:
      result.append(cat_2_id[ex])
  return np.array(result)

Y_train_id = preprocess_y(Y_train, cat_2_id)
Y_test_id = preprocess_y(Y_test, cat_2_id)

"""## Tokenizer les titres

Pour cela vous pouvez utiliser la fonction `Tokenizer` de keras

Le but est de transformer les textes en un vecteur numérique

texte -> liste de tokens -> vecteur numérique

"Miaou le chat" -> ["Miaou", "le", chat"] -> [1, 2, 3]
"""

# Créer le tokenizer
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)

# Entrainer le tokenizer sur le train set 
tokenizer.fit_on_texts(X_train)
print(tokenizer.word_counts)

# Transformer les textes en vecteurs numeriques à l'aide du tokenizer
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

"""## Faire un padding des sequences obtenues pour qu'elles aient toutes la même taille (cf la fonction `pad_sequences`)

[1, 2, 3]       -> [0, 0, 1, 2 ,3]

[4, 5, 6, 7, 8] -> [4, 5, 6, 7, 8]
"""

# Padding des sequences 
X_train_seq = tf.keras.preprocessing.sequence.pad_sequences(X_train_seq, maxlen= 40,truncating='post')

X_test_seq = tf.keras.preprocessing.sequence.pad_sequences(X_test_seq, maxlen= 40,truncating='post')

X_train_seq[0:5]

X_test_seq[0:5]

Y_test_id[0:100]

"""# Réseau de convolution pour la classification de texte

Les réseaux convolutionnels peuvent également être utiliser pour le texte et notamment pour la classification de texte. Ici nous allons construire un CNN sur le même modèle que pour les images avec quelques petites spécificités. 

Comme le texte est une séquence de mots, il s'agit d'une séquence en 1 dimension. Nous appliquerons donc une convolution en 1D. 

Pour traiter du texte, la première couche de notre réseau va être constituée par une couche d'embedding. 

Pour rappel, le word embedding consiste à projeter les tokens dans un espace vectoriel qui va minimiser la distance entre les tokens qui sont utilisés dans des contextes similaires (et qui ont un sens proche ? )

![Texte alternatif…](https://www.ibm.com/blogs/research/wp-content/uploads/2018/10/WMEFig1.png)

Les embeddings peuvent être calculés de diverses façons. Par exemple word2vec, un des plus célèbres, se base sur 2 algorithmes frères Skip-gram et CBOW

![Texte alternatif…](https://pathmind.com/images/wiki/word2vec_diagrams.png)

Pour information, il existe aujourd'hui des algorithmes plus performants que word2vec comme [Fasttext](https://fasttext.cc) qui prend en compte des informations de sous-mots ou la famille des embeddings contextuels comme [ELMo](https://allennlp.org/elmo) ou [BERT](https://arxiv.org/abs/1810.04805) qui prennent en compte le contexte d'utilisation du mot pour calculer son vecteur.
"""

# Créer le modèle avec au minimum
# Embedding 
# Dropout
# Convolution
# Maxpooling
# Dense
# Activation
# Classifieur (Dense + activation softmax)

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Embedding(10000, 100, input_length = 40))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Conv1D(32, 2, activation="relu"))
model.add(tf.keras.layers.GlobalMaxPooling1D())
model.add(tf.keras.layers.Dense(128, activation="relu"))
model.add(tf.keras.layers.Dense(cat_vocab+1, activation="softmax"))
# Compiler le modèle 
model.compile(loss = "sparse_categorical_crossentropy",
              optimizer="adam",
              metrics= "accuracy")
# Afficher le summary du modèle
model.summary()

# Fitter le modèle 
model.fit(X_train_seq, Y_train_id, batch_size = 128, epochs = 10 )

# Evaluer le modèle
score = model.evaluate(X_test_seq, Y_test_id)
print('Test score:', score[0])
print('Test accuracy:', score[1])

"""# LSTM pour la classification de texte

Il est également possible d'utiliser un autre type de réseau de neurones pour effectuer ce genre de tâches: les réseaux de neurones récurrents ou RNN.

Les RNN sont conçus pour gérer les séquences. Le réseau prend les tokens un par un et calcule une représentation de la séquence à chaque pas qui tiens compte de tous les pas précédents 

![Texte alternatif…](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/450px-Recurrent_neural_network_unfold.svg.png)


Il existe différents types de RNN. Ici nous utiliserons les Long Short-Term Memory (LSTM) qui permettent d'améliorer les performances sur des séquences longues avec une série de "gates". 

![Texte alternatif…](http://dprogrammer.org/wp-content/uploads/2019/04/RNN-vs-LSTM-vs-GRU-1200x361.png)
"""

# Créer un réseau à base de LSTM avec au minimum:
# Embedding
# Dropout
# LSTM
# Dropout
# Classifieur
model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Embedding(10000, 100, input_length = 40))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.LSTM(128, activation="tanh", recurrent_activation="sigmoid"))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(cat_vocab+1, activation="softmax"))

# Compiler le modèle 
model.compile(loss = "sparse_categorical_crossentropy",
              optimizer="adam",
              metrics= "accuracy")
# Afficher le summary du modèle

# Fitter le modèle
model.fit(X_train_seq, Y_train_id, batch_size = 128, epochs = 5 )

# Evaluer le modèle

"""# Utiliser les embeddings pré-entrainés

Pour améliorer la qualité de la représentation des mots, il est possible d'entrainer les embeddings sur de larges corpus de textes non annotés (typiquement Wikipedia). Ces modèles sont souvent disponibles en ligne et il est possible de les télécharger. Ici nous allons utiliser des embeddings [Glove](https://nlp.stanford.edu/projects/glove/) de taille 50d (pour des raisons techniques mais il vaut mieux utiliser des dimensions plus importantes entre 100 et 300) 
"""

# Fonction permettant de charger un embedding 

import numpy as np
import re
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

def load_glove_embeddings(fp, embedding_dim, include_empty_char=True):
    """
    Loads pre-trained word embeddings (GloVe embeddings)
        Inputs: - fp: filepath of pre-trained glove embeddings
                - embedding_dim: dimension of each vector embedding
                - generate_matrix: whether to generate an embedding matrix
        Outputs:
                - word2coefs: Dictionary. Word to its corresponding coefficients
                - word2index: Dictionary. Word to word-index
                - embedding_matrix: Embedding matrix for Keras Embedding layer
    """
    # First, build the "word2coefs" and "word2index"
    word2coefs = {} # word to its corresponding coefficients
    word2index = {} # word to word-index
    with open(fp) as f:
        for idx, line in enumerate(f):
            try:
                data = [x.strip().lower() for x in line.split()]
                word = data[0]
                coefs = np.asarray(data[1:embedding_dim+1], dtype='float32')
                word2coefs[word] = coefs
                if word not in word2index:
                    word2index[word] = len(word2index)
            except Exception as e:
                print('Exception occurred in `load_glove_embeddings`:', e)
                continue
        # End of for loop.
    # End of with open
    if include_empty_char:
        word2index[''] = len(word2index)
    # Second, build the "embedding_matrix"
    # Words not found in embedding index will be all-zeros. Hence, the "+1".
    vocab_size = len(word2coefs)+1 if include_empty_char else len(word2coefs)
    embedding_matrix = np.zeros((vocab_size, embedding_dim))
    for word, idx in word2index.items():
        embedding_vec = word2coefs.get(word)
        if embedding_vec is not None and embedding_vec.shape[0]==embedding_dim:
            embedding_matrix[idx] = np.asarray(embedding_vec)
    # return word2coefs, word2index, embedding_matrix
    return word2index, np.asarray(embedding_matrix)



# Télécharger les embeddings

!wget https://github.com/kmr0877/IMDB-Sentiment-Classification-CBOW-Model/raw/master/glove.6B.50d.txt.gz
!gunzip /content/glove.6B.50d.txt.gz

# Charger les embeddings à l'aide de la fonction load_glove_embeddings

# ecrire une fonction de tokenization custom pour preprocesser les textes


# Encoder les textes avec la fonction custom

# Padding des sequences

# Créer un modèle en chargeant les poids des embeddings dans le layer Embedding

# Fitter le modèle

# evaluer le modèle